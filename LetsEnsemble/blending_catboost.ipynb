{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92d257cd-824a-4921-8b21-c14bada43824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/sarmat/lgbm-stacking-example/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cc2c9a-80ee-4932-b759-7f49c34b9ee2",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.9 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import RocCurveDisplay, accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from dataset import custom_train_test_split, make_dataset\n",
    "\n",
    "from sklearn.metrics import RocCurveDisplay, accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "\n",
    "\n",
    "def get_metric(targets, preds):\n",
    "    auc = roc_auc_score(targets, preds)\n",
    "    acc = accuracy_score(targets, np.where(preds >= 0.5, 1, 0))\n",
    "    precsion = precision_score(targets, np.where(preds >= 0.5, 1, 0))\n",
    "    recall = recall_score(targets, np.where(preds >= 0.5, 1, 0))\n",
    "    F1_score = f1_score(targets, np.where(preds >= 0.5, 1, 0))\n",
    "\n",
    "    print('auc :',auc)\n",
    "    print('acc :',acc)\n",
    "    print('precision :',precsion)\n",
    "    print('recall :',recall)\n",
    "\n",
    "def test_to_csv(preds, name:str):\n",
    "    \n",
    "    result = []\n",
    "    for n,i in enumerate(preds):\n",
    "        row = {}    \n",
    "        row['id'] = n\n",
    "        row['prediction'] = i\n",
    "        result.append(row)\n",
    "    pd.DataFrame(result).to_csv(f'output/{name}.csv', index=None)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7f6473-3745-4b22-9fbd-f6dfe18b4a5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.9 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "cate_cols = [\n",
    "            'assessmentItemID',\n",
    "            'testId',\n",
    "            'KnowledgeTag',\n",
    "            'hour',\n",
    "            'dow',\n",
    "            # 'i_head',\n",
    "            # 'i_mid',\n",
    "            # 'i_tail',\n",
    "]\n",
    "cont_cols = [                        \n",
    "            'user_correct_answer',\n",
    "            'user_total_answer',\n",
    "            'user_acc',            \n",
    "            't_elapsed',            \n",
    "            'cum_correct',\n",
    "            'last_problem',\n",
    "            'head_term',\n",
    "            # 'left_asymptote',\n",
    "            'elo_prob',\n",
    "            'pkt',\n",
    "            'u_head_mean',\n",
    "            'u_head_count',\n",
    "            'u_head_std',\n",
    "            'u_head_elapsed',\n",
    "            'i_mid_elapsed',\n",
    "            'i_mid_mean',\n",
    "            'i_mid_std',\n",
    "            'i_mid_sum',\n",
    "            'i_mid_count',\n",
    "            'i_mid_tag_count',\n",
    "            # 'assessment_mean',\n",
    "            # 'assessment_sum',\n",
    "            # 'assessment_std',\n",
    "            'tag_mean',\n",
    "            'tag_sum',\n",
    "            # 'tag_std',\n",
    "            'tail_mean',\n",
    "            'tail_sum',\n",
    "            # 'tail_std',\n",
    "            'hour_mean',\n",
    "            'hour_sum',\n",
    "            # 'hour_std',\n",
    "            'dow_mean',\n",
    "            'dow_sum',\n",
    "            # 'dow_std',\n",
    "            'tag_elapsed',\n",
    "            'tag_elapsed_o',\n",
    "            'tag_elapsed_x',\n",
    "            'assessment_elapsed',\n",
    "            'assessment_elapsed_o',\n",
    "            'assessment_elapsed_x',\n",
    "            'tail_elapsed',\n",
    "            'tail_elapsed_o',\n",
    "            'tail_elapsed_x'\n",
    "            ]\n",
    "\n",
    "FEATS = cate_cols + cont_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d820d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_pickle('/opt/ml/level2-dkt-level2-recsys-08/data_pkl/train_data.pkl')\n",
    "valid_user = pd.read_csv('/opt/ml/input/data/cv_valid_data.csv').userID.unique()\n",
    "from dataset import feature_engineering, custom_train_test_split, make_dataset\n",
    "\n",
    "\n",
    "train = train_data[train_data.userID.isin(valid_user)==False]\n",
    "valid = train_data[train_data.userID.isin(valid_user)==True]\n",
    "\n",
    "y_train, x_train, y_valid, x_valid = make_dataset(train, valid)\n",
    "\n",
    "test = pd.read_pickle('/opt/ml/level2-dkt-level2-recsys-08/data_pkl/test_data-1.pkl')\n",
    "test = test[test.answerCode==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c404f1b8-04c6-49bf-843b-3f3485fd513a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc : 0.8583931229778012\n",
      "acc : 0.8016628808308105\n",
      "precision : 0.8181892949962167\n",
      "recall : 0.8965303854125988\n",
      "AUC LGB1:None \n",
      "auc : 0.8540646557294791\n",
      "acc : 0.7985371057842585\n",
      "precision : 0.8149935509258247\n",
      "recall : 0.8959175703685902\n",
      "AUC LGB2:None \n",
      "auc : 0.844303795765355\n",
      "acc : 0.7925414440765776\n",
      "precision : 0.8066700816909762\n",
      "recall : 0.8988014058698068\n",
      "AUC LGB3:None \n",
      "auc : 0.8553643567087972\n",
      "acc : 0.7994858611825193\n",
      "precision : 0.8159543099722644\n",
      "recall : 0.8961158340592988\n",
      "AUC LGB4:None \n",
      "auc : 0.8441299857257601\n",
      "acc : 0.7923761008121504\n",
      "precision : 0.8064279669923948\n",
      "recall : 0.8989095497011025\n",
      "AUC LGB5:None \n"
     ]
    }
   ],
   "source": [
    "train_pool = Pool(x_train[FEATS] ,y_train, cat_features = cate_cols)\n",
    "eval_pool = Pool(x_valid[FEATS] , y_valid, cat_features = cate_cols)\n",
    "\n",
    "# num_round? 1000~ 10000\n",
    "model1 = CatBoostClassifier(\n",
    "        iterations = 2000,\n",
    "        random_seed = 42,\n",
    "        learning_rate = 0.01,\n",
    "        loss_function = 'Logloss', \n",
    "        custom_metric = ['Logloss','AUC'],\n",
    "        early_stopping_rounds = 30,\n",
    "        use_best_model =  True,\n",
    "        task_type = \"GPU\",\n",
    "        bagging_temperature = 1,\n",
    "        verbose = False)\n",
    "\n",
    "model2 = CatBoostClassifier(\n",
    "        iterations = 2000,\n",
    "        random_seed = 42,\n",
    "        learning_rate = 0.005,\n",
    "        loss_function = 'Logloss', \n",
    "        custom_metric = ['Logloss','AUC'],\n",
    "        early_stopping_rounds = 30,\n",
    "        use_best_model =  True,\n",
    "        task_type = \"GPU\",\n",
    "        bagging_temperature = 1,\n",
    "        verbose = False)\n",
    "\n",
    "model3 = CatBoostClassifier(\n",
    "        iterations = 2000,\n",
    "        random_seed = 42,\n",
    "        learning_rate = 0.001,\n",
    "        loss_function = 'Logloss', \n",
    "        custom_metric = ['Logloss','AUC'],\n",
    "        early_stopping_rounds = 30,\n",
    "        use_best_model =  True,\n",
    "        task_type = \"GPU\",\n",
    "        bagging_temperature = 1,\n",
    "        verbose = False)\n",
    "\n",
    "model4 = CatBoostClassifier(\n",
    "        iterations = 2000,\n",
    "        random_seed = 35,\n",
    "        learning_rate = 0.003,\n",
    "        loss_function = 'Logloss', \n",
    "        custom_metric = ['Logloss','AUC'],\n",
    "        early_stopping_rounds = 30,\n",
    "        use_best_model =  True,\n",
    "        task_type = \"GPU\",\n",
    "        bagging_temperature = 1,\n",
    "        verbose = False)\n",
    "\n",
    "model5 = CatBoostClassifier(\n",
    "        iterations = 2000,\n",
    "        random_seed = 2020,\n",
    "        learning_rate = 0.001,\n",
    "        loss_function = 'Logloss', \n",
    "        custom_metric = ['Logloss','AUC'],\n",
    "        early_stopping_rounds = 30,\n",
    "        use_best_model =  True,\n",
    "        task_type = \"GPU\",\n",
    "        bagging_temperature = 1,\n",
    "        verbose = False)\n",
    "\n",
    "model1.fit(train_pool, eval_set=eval_pool,plot=True)\n",
    "model2.fit(train_pool, eval_set=eval_pool,plot=True)\n",
    "model3.fit(train_pool, eval_set=eval_pool,plot=True)\n",
    "model4.fit(train_pool, eval_set=eval_pool,plot=True)\n",
    "model5.fit(train_pool, eval_set=eval_pool,plot=True)\n",
    "\n",
    "test_preds1 = model1.predict(test[FEATS], prediction_type='Probability')[:,1]\n",
    "test_preds2 = model2.predict(test[FEATS], prediction_type='Probability')[:,1]\n",
    "test_preds3 = model3.predict(test[FEATS], prediction_type='Probability')[:,1]\n",
    "test_preds4 = model4.predict(test[FEATS], prediction_type='Probability')[:,1]\n",
    "test_preds5 = model5.predict(test[FEATS], prediction_type='Probability')[:,1]\n",
    "\n",
    "valid_preds1 = model1.predict(x_valid[FEATS], prediction_type='Probability')[:,1]\n",
    "valid_preds2 = model2.predict(x_valid[FEATS], prediction_type='Probability')[:,1]\n",
    "valid_preds3 = model3.predict(x_valid[FEATS], prediction_type='Probability')[:,1]\n",
    "valid_preds4 = model4.predict(x_valid[FEATS], prediction_type='Probability')[:,1]\n",
    "valid_preds5 = model5.predict(x_valid[FEATS], prediction_type='Probability')[:,1]\n",
    "\n",
    "\n",
    "# print('Fold no: {}'.format(fold_))\n",
    "print(\"AUC LGB1:{} \".format(get_metric(y_valid, valid_preds1)))\n",
    "print(\"AUC LGB2:{} \".format(get_metric(y_valid, valid_preds2)))\n",
    "print(\"AUC LGB3:{} \".format(get_metric(y_valid, valid_preds3)))\n",
    "print(\"AUC LGB4:{} \".format(get_metric(y_valid, valid_preds4)))\n",
    "print(\"AUC LGB5:{} \".format(get_metric(y_valid, valid_preds5))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6882bc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_valid = x_valid[FEATS].copy()\n",
    "new_valid.loc[:,'predict1'] = valid_preds1\n",
    "new_valid.loc[:,'predict2'] = valid_preds2\n",
    "new_valid.loc[:,'predict3'] = valid_preds3\n",
    "new_valid.loc[:,'predict4'] = valid_preds4\n",
    "new_valid.loc[:,'predict5'] = valid_preds5\n",
    "\n",
    "valid_tail = new_valid[new_valid.index.isin(x_valid.groupby('userID').tail(1).index)==True]\n",
    "new_valid = new_valid[new_valid.index.isin(x_valid.groupby('userID').tail(1).index)==False]\n",
    "\n",
    "new_test = test[FEATS].copy()\n",
    "new_test.loc[:,'predict1'] = test_preds1\n",
    "new_test.loc[:,'predict2'] = test_preds2\n",
    "new_test.loc[:,'predict3'] = test_preds3\n",
    "new_test.loc[:,'predict4'] = test_preds4\n",
    "new_test.loc[:,'predict5'] = test_preds5\n",
    "\n",
    "FEATS += [\n",
    "        'predict1',\n",
    "        'predict2',\n",
    "        'predict3',\n",
    "        'predict4',\n",
    "        'predict5',\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c2cca4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8327116-8598-4455-b695-9d18c7e88675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9a722a9dd7046c38d5e7ab29af039bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc : 0.8565130666049953\n",
      "acc : 0.7701612903225806\n",
      "precision : 0.7790368271954674\n",
      "recall : 0.7472826086956522\n"
     ]
    }
   ],
   "source": [
    "y_tail = y_valid[y_valid.index.isin(x_valid.groupby('userID').tail(1).index)==True]\n",
    "y_new_valid = y_valid[y_valid.index.isin(x_valid.groupby('userID').tail(1).index)==False]\n",
    "\n",
    "train_pool = Pool(new_valid[FEATS] ,y_new_valid, cat_features = cate_cols)\n",
    "eval_pool = Pool(valid_tail[FEATS] , y_tail, cat_features = cate_cols)\n",
    "\n",
    "\n",
    "Final_cat = CatBoostClassifier(\n",
    "            iterations = 500,\n",
    "            random_seed = 42,\n",
    "            learning_rate = 0.002,\n",
    "            loss_function = 'Logloss', \n",
    "            custom_metric = ['Logloss','AUC'],\n",
    "            early_stopping_rounds = 30,\n",
    "            use_best_model =  True,\n",
    "            task_type = \"GPU\",\n",
    "            bagging_temperature = 1,\n",
    "            verbose = False)\n",
    "\n",
    "Final_cat.fit(train_pool, eval_set=eval_pool, plot=True)\n",
    "\n",
    "Final_valid_preds = Final_cat.predict(valid_tail, prediction_type='Probability')[:,1]\n",
    "Final_test_preds = Final_cat.predict(new_test, prediction_type='Probability')[:,1]\n",
    "\n",
    "\n",
    "# print('Fold no: {}'.format(fold_))\n",
    "get_metric(y_tail, Final_valid_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e9d7861-8ecc-4520-81bf-6d930f32a634",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, datetime, timezone, timedelta\n",
    "\n",
    "KST = timezone(timedelta(hours=9))\n",
    "time_record = datetime.now(KST)\n",
    "_day = str(time_record)[:10]\n",
    "_time = str(time_record.time())[:8]\n",
    "now_time = _day+'_'+_time\n",
    "\n",
    "test_to_csv(Final_test_preds,f'belnding_catboost_{now_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf5517b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
