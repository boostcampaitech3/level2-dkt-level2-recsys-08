{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92d257cd-824a-4921-8b21-c14bada43824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/sarmat/lgbm-stacking-example/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1cc2c9a-80ee-4932-b759-7f49c34b9ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from dataset import custom_train_test_split, make_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84f4ceb8-7f94-43ca-90e5-1fa66095051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "param1 = {   \n",
    "            'boosting': 'gbdt',\n",
    "            'objective': 'binary',\n",
    "            'num_leaves' : 30,\n",
    "            'metric' : 'binary_logloss',\n",
    "            'learning_rate' : 0.1\n",
    "            \n",
    "            }\n",
    "\n",
    "param2 = {   \n",
    "            'boosting': 'gbdt', \n",
    "            # 'max_depth' : 10,\n",
    "            'objective': 'binary',\n",
    "            'num_leaves' : 50, # 30 ~ 60\n",
    "            'metric' : 'binary_logloss',\n",
    "            'learning_rate' : 0.1 #  0.01~0.3\n",
    "            }\n",
    "\n",
    "param3 = {   \n",
    "            'boosting': 'gbdt',\n",
    "            'objective': 'binary',\n",
    "            'num_leaves' : 48,\n",
    "            'metric' : 'binary_logloss',\n",
    "            # 'learning_rate' : 0.005\n",
    "            }\n",
    "\n",
    "param4 = {   \n",
    "            'boosting': 'gbdt',\n",
    "            'objective': 'binary',\n",
    "            'num_leaves' : 60,\n",
    "            'metric' : 'binary_logloss',\n",
    "            # 'learning_rate' : 0.005\n",
    "            }\n",
    "\n",
    "param5 = {   \n",
    "            'boosting': 'gbdt',\n",
    "            'objective': 'binary',\n",
    "            'num_leaves' : 70,\n",
    "            'metric' : 'binary_logloss',\n",
    "            # 'learning_rate' : 0.01\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee7f6473-3745-4b22-9fbd-f6dfe18b4a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['i_head', 'i_mid','i_tail', 'hour', 'dow']\n",
    "cont_cols = [                        \n",
    "        'user_correct_answer',\n",
    "        'user_total_answer',\n",
    "        'user_acc',            \n",
    "        't_elapsed',            \n",
    "        'cum_correct',\n",
    "        'last_problem',\n",
    "        'head_term',\n",
    "        # 'left_asymptote',\n",
    "        'elo_prob',\n",
    "        'pkt',\n",
    "        'u_head_mean',\n",
    "        'u_head_count',\n",
    "        'u_head_std',\n",
    "        'u_head_elapsed',\n",
    "        'i_mid_elapsed',\n",
    "        'i_mid_mean',\n",
    "        'i_mid_std',\n",
    "        'i_mid_sum',\n",
    "        'i_mid_count',\n",
    "        'i_mid_tag_count',\n",
    "        'assessment_mean',\n",
    "        'assessment_sum',\n",
    "        # 'assessment_std',\n",
    "        'tag_mean',\n",
    "        'tag_sum',\n",
    "        # 'tag_std',\n",
    "        'tail_mean',\n",
    "        'tail_sum',\n",
    "        # 'tail_std',\n",
    "        'hour_mean',\n",
    "        'hour_sum',\n",
    "        # 'hour_std',\n",
    "        'dow_mean',\n",
    "        'dow_sum',\n",
    "        # 'dow_std',\n",
    "        'tag_elapsed',\n",
    "        'tag_elapsed_o',\n",
    "        'tag_elapsed_x',\n",
    "        'assessment_elapsed',\n",
    "        'assessment_elapsed_o',\n",
    "        'assessment_elapsed_x',\n",
    "        'tail_elapsed',\n",
    "        'tail_elapsed_o',\n",
    "        'tail_elapsed_x']\n",
    "\n",
    "FEATS = cat_cols + cont_cols    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9884ba83-6b5d-425b-be04-c0000a1f3c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('/opt/ml/level2-dkt-level2-recsys-08/data_pkl/all.pkl')\n",
    "y = train.answerCode\n",
    "# train, test = custom_train_test_split(train_data)\n",
    "# y_train, train, y_test, test = make_dataset(train, test)\n",
    "test = pd.read_pickle('/opt/ml/level2-dkt-level2-recsys-08/data_pkl/test_data.pkl')\n",
    "test = test[test.answerCode==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c404f1b8-04c6-49bf-843b-3f3485fd513a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/opt/conda/lib/python3.8/site-packages/lightgbm/basic.py:2068: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['dow', 'hour', 'i_head', 'i_mid', 'i_tail']\n",
      "  _log_warning('categorical_feature in Dataset is overridden.\\n'\n",
      "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1102675, number of negative: 581295\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038236 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6623\n",
      "[LightGBM] [Info] Number of data points in the train set: 1683970, number of used features: 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/opt/conda/lib/python3.8/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654807 -> initscore=0.640236\n",
      "[LightGBM] [Info] Start training from score 0.640236\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.43051\n",
      "[200]\tvalid_0's binary_logloss: 0.427988\n",
      "[300]\tvalid_0's binary_logloss: 0.427163\n",
      "[400]\tvalid_0's binary_logloss: 0.426864\n",
      "[500]\tvalid_0's binary_logloss: 0.426688\n",
      "[600]\tvalid_0's binary_logloss: 0.426551\n",
      "[700]\tvalid_0's binary_logloss: 0.426398\n",
      "[800]\tvalid_0's binary_logloss: 0.426277\n",
      "[900]\tvalid_0's binary_logloss: 0.426296\n",
      "[1000]\tvalid_0's binary_logloss: 0.426253\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[974]\tvalid_0's binary_logloss: 0.426219\n",
      "[LightGBM] [Info] Number of positive: 1102675, number of negative: 581295\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041778 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6623\n",
      "[LightGBM] [Info] Number of data points in the train set: 1683970, number of used features: 43\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654807 -> initscore=0.640236\n",
      "[LightGBM] [Info] Start training from score 0.640236\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.429517\n",
      "[200]\tvalid_0's binary_logloss: 0.427446\n",
      "[300]\tvalid_0's binary_logloss: 0.426873\n",
      "[400]\tvalid_0's binary_logloss: 0.426565\n",
      "[500]\tvalid_0's binary_logloss: 0.426495\n",
      "[600]\tvalid_0's binary_logloss: 0.426396\n",
      "[700]\tvalid_0's binary_logloss: 0.426291\n",
      "[800]\tvalid_0's binary_logloss: 0.426243\n",
      "[900]\tvalid_0's binary_logloss: 0.426257\n",
      "Early stopping, best iteration is:\n",
      "[870]\tvalid_0's binary_logloss: 0.426211\n",
      "[LightGBM] [Info] Number of positive: 1102675, number of negative: 581295\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039342 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6623\n",
      "[LightGBM] [Info] Number of data points in the train set: 1683970, number of used features: 43\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654807 -> initscore=0.640236\n",
      "[LightGBM] [Info] Start training from score 0.640236\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.429101\n",
      "[200]\tvalid_0's binary_logloss: 0.427154\n",
      "[300]\tvalid_0's binary_logloss: 0.426746\n",
      "[400]\tvalid_0's binary_logloss: 0.426586\n",
      "[500]\tvalid_0's binary_logloss: 0.426446\n",
      "[600]\tvalid_0's binary_logloss: 0.426417\n",
      "[700]\tvalid_0's binary_logloss: 0.426372\n",
      "[800]\tvalid_0's binary_logloss: 0.426344\n",
      "Early stopping, best iteration is:\n",
      "[780]\tvalid_0's binary_logloss: 0.4263\n",
      "[LightGBM] [Info] Number of positive: 1102675, number of negative: 581295\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069976 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6623\n",
      "[LightGBM] [Info] Number of data points in the train set: 1683970, number of used features: 43\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654807 -> initscore=0.640236\n",
      "[LightGBM] [Info] Start training from score 0.640236\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.428162\n",
      "[200]\tvalid_0's binary_logloss: 0.426767\n",
      "[300]\tvalid_0's binary_logloss: 0.42657\n",
      "[400]\tvalid_0's binary_logloss: 0.426418\n",
      "[500]\tvalid_0's binary_logloss: 0.426432\n",
      "Early stopping, best iteration is:\n",
      "[401]\tvalid_0's binary_logloss: 0.42641\n",
      "[LightGBM] [Info] Number of positive: 1102675, number of negative: 581295\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057464 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6623\n",
      "[LightGBM] [Info] Number of data points in the train set: 1683970, number of used features: 43\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654807 -> initscore=0.640236\n",
      "[LightGBM] [Info] Start training from score 0.640236\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.427857\n",
      "[200]\tvalid_0's binary_logloss: 0.426621\n",
      "[300]\tvalid_0's binary_logloss: 0.426376\n",
      "[400]\tvalid_0's binary_logloss: 0.42624\n",
      "[500]\tvalid_0's binary_logloss: 0.426285\n",
      "Early stopping, best iteration is:\n",
      "[452]\tvalid_0's binary_logloss: 0.426213\n",
      "AUC LGB1:0.8655615787620669 \n",
      "AUC LGB2:0.8655457381537353 \n",
      "AUC LGB3:0.8654923739244257 \n",
      "AUC LGB4:0.865425796941456 \n",
      "AUC LGB5:0.8655451548138007 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/opt/conda/lib/python3.8/site-packages/lightgbm/basic.py:2068: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['dow', 'hour', 'i_head', 'i_mid', 'i_tail']\n",
      "  _log_warning('categorical_feature in Dataset is overridden.\\n'\n",
      "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1101885, number of negative: 582086\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036678 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6622\n",
      "[LightGBM] [Info] Number of data points in the train set: 1683971, number of used features: 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/opt/conda/lib/python3.8/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654337 -> initscore=0.638159\n",
      "[LightGBM] [Info] Start training from score 0.638159\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.429782\n",
      "[200]\tvalid_0's binary_logloss: 0.427361\n",
      "[300]\tvalid_0's binary_logloss: 0.426542\n",
      "[400]\tvalid_0's binary_logloss: 0.426205\n",
      "[500]\tvalid_0's binary_logloss: 0.425999\n",
      "[600]\tvalid_0's binary_logloss: 0.425907\n",
      "[700]\tvalid_0's binary_logloss: 0.42588\n",
      "[800]\tvalid_0's binary_logloss: 0.425796\n",
      "Early stopping, best iteration is:\n",
      "[786]\tvalid_0's binary_logloss: 0.425787\n",
      "[LightGBM] [Info] Number of positive: 1101885, number of negative: 582086\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055203 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6622\n",
      "[LightGBM] [Info] Number of data points in the train set: 1683971, number of used features: 43\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654337 -> initscore=0.638159\n",
      "[LightGBM] [Info] Start training from score 0.638159\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.428873\n",
      "[200]\tvalid_0's binary_logloss: 0.426854\n",
      "[300]\tvalid_0's binary_logloss: 0.426263\n",
      "[400]\tvalid_0's binary_logloss: 0.426014\n",
      "[500]\tvalid_0's binary_logloss: 0.425859\n",
      "[600]\tvalid_0's binary_logloss: 0.425824\n",
      "[700]\tvalid_0's binary_logloss: 0.42578\n",
      "[800]\tvalid_0's binary_logloss: 0.425851\n",
      "Early stopping, best iteration is:\n",
      "[711]\tvalid_0's binary_logloss: 0.425776\n",
      "[LightGBM] [Info] Number of positive: 1101885, number of negative: 582086\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.076845 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6622\n",
      "[LightGBM] [Info] Number of data points in the train set: 1683971, number of used features: 43\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654337 -> initscore=0.638159\n",
      "[LightGBM] [Info] Start training from score 0.638159\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.428112\n",
      "[200]\tvalid_0's binary_logloss: 0.426345\n",
      "[300]\tvalid_0's binary_logloss: 0.425721\n",
      "[400]\tvalid_0's binary_logloss: 0.425479\n",
      "[500]\tvalid_0's binary_logloss: 0.425339\n",
      "[600]\tvalid_0's binary_logloss: 0.425387\n",
      "Early stopping, best iteration is:\n",
      "[521]\tvalid_0's binary_logloss: 0.425317\n",
      "[LightGBM] [Info] Number of positive: 1101885, number of negative: 582086\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040504 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6622\n",
      "[LightGBM] [Info] Number of data points in the train set: 1683971, number of used features: 43\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654337 -> initscore=0.638159\n",
      "[LightGBM] [Info] Start training from score 0.638159\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.427494\n",
      "[200]\tvalid_0's binary_logloss: 0.426082\n",
      "[300]\tvalid_0's binary_logloss: 0.425818\n",
      "[400]\tvalid_0's binary_logloss: 0.425562\n",
      "[500]\tvalid_0's binary_logloss: 0.425561\n",
      "[600]\tvalid_0's binary_logloss: 0.425519\n",
      "Early stopping, best iteration is:\n",
      "[552]\tvalid_0's binary_logloss: 0.425493\n",
      "[LightGBM] [Info] Number of positive: 1101885, number of negative: 582086\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056965 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6622\n",
      "[LightGBM] [Info] Number of data points in the train set: 1683971, number of used features: 43\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654337 -> initscore=0.638159\n",
      "[LightGBM] [Info] Start training from score 0.638159\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.427106\n",
      "[200]\tvalid_0's binary_logloss: 0.426125\n",
      "[300]\tvalid_0's binary_logloss: 0.425814\n",
      "[400]\tvalid_0's binary_logloss: 0.42582\n",
      "Early stopping, best iteration is:\n",
      "[318]\tvalid_0's binary_logloss: 0.425776\n",
      "AUC LGB1:0.8655734706367643 \n",
      "AUC LGB2:0.8655580032655293 \n",
      "AUC LGB3:0.8658420000256191 \n",
      "AUC LGB4:0.8656923170578461 \n",
      "AUC LGB5:0.8655183166041627 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/opt/conda/lib/python3.8/site-packages/lightgbm/basic.py:2068: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['dow', 'hour', 'i_head', 'i_mid', 'i_tail']\n",
      "  _log_warning('categorical_feature in Dataset is overridden.\\n'\n",
      "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1102616, number of negative: 581355\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.046860 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6623\n",
      "[LightGBM] [Info] Number of data points in the train set: 1683971, number of used features: 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/opt/conda/lib/python3.8/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654771 -> initscore=0.640079\n",
      "[LightGBM] [Info] Start training from score 0.640079\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.430234\n",
      "[200]\tvalid_0's binary_logloss: 0.427705\n",
      "[300]\tvalid_0's binary_logloss: 0.426869\n",
      "[400]\tvalid_0's binary_logloss: 0.426439\n",
      "[500]\tvalid_0's binary_logloss: 0.426275\n",
      "[600]\tvalid_0's binary_logloss: 0.426104\n",
      "[700]\tvalid_0's binary_logloss: 0.425958\n",
      "[800]\tvalid_0's binary_logloss: 0.425907\n",
      "Early stopping, best iteration is:\n",
      "[778]\tvalid_0's binary_logloss: 0.425878\n",
      "[LightGBM] [Info] Number of positive: 1102616, number of negative: 581355\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041464 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6623\n",
      "[LightGBM] [Info] Number of data points in the train set: 1683971, number of used features: 43\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654771 -> initscore=0.640079\n",
      "[LightGBM] [Info] Start training from score 0.640079\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.429122\n",
      "[200]\tvalid_0's binary_logloss: 0.426934\n",
      "[300]\tvalid_0's binary_logloss: 0.426452\n",
      "[400]\tvalid_0's binary_logloss: 0.426235\n",
      "[500]\tvalid_0's binary_logloss: 0.426028\n",
      "[600]\tvalid_0's binary_logloss: 0.425951\n",
      "[700]\tvalid_0's binary_logloss: 0.425956\n",
      "[800]\tvalid_0's binary_logloss: 0.425915\n",
      "Early stopping, best iteration is:\n",
      "[756]\tvalid_0's binary_logloss: 0.425871\n",
      "[LightGBM] [Info] Number of positive: 1102616, number of negative: 581355\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040760 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6623\n",
      "[LightGBM] [Info] Number of data points in the train set: 1683971, number of used features: 43\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654771 -> initscore=0.640079\n",
      "[LightGBM] [Info] Start training from score 0.640079\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.428601\n",
      "[200]\tvalid_0's binary_logloss: 0.426801\n",
      "[300]\tvalid_0's binary_logloss: 0.426513\n",
      "[400]\tvalid_0's binary_logloss: 0.426252\n",
      "[500]\tvalid_0's binary_logloss: 0.426097\n",
      "[600]\tvalid_0's binary_logloss: 0.426034\n",
      "[700]\tvalid_0's binary_logloss: 0.425833\n",
      "[800]\tvalid_0's binary_logloss: 0.42594\n",
      "Early stopping, best iteration is:\n",
      "[705]\tvalid_0's binary_logloss: 0.42582\n",
      "[LightGBM] [Info] Number of positive: 1102616, number of negative: 581355\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041646 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6623\n",
      "[LightGBM] [Info] Number of data points in the train set: 1683971, number of used features: 43\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654771 -> initscore=0.640079\n",
      "[LightGBM] [Info] Start training from score 0.640079\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.427971\n",
      "[200]\tvalid_0's binary_logloss: 0.426488\n",
      "[300]\tvalid_0's binary_logloss: 0.426103\n",
      "[400]\tvalid_0's binary_logloss: 0.426081\n",
      "Early stopping, best iteration is:\n",
      "[318]\tvalid_0's binary_logloss: 0.426047\n",
      "[LightGBM] [Info] Number of positive: 1102616, number of negative: 581355\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043695 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6623\n",
      "[LightGBM] [Info] Number of data points in the train set: 1683971, number of used features: 43\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654771 -> initscore=0.640079\n",
      "[LightGBM] [Info] Start training from score 0.640079\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.427746\n",
      "[200]\tvalid_0's binary_logloss: 0.426357\n",
      "[300]\tvalid_0's binary_logloss: 0.426169\n",
      "[400]\tvalid_0's binary_logloss: 0.426002\n",
      "[500]\tvalid_0's binary_logloss: 0.426012\n",
      "Early stopping, best iteration is:\n",
      "[450]\tvalid_0's binary_logloss: 0.425953\n",
      "AUC LGB1:0.8658564971331191 \n",
      "AUC LGB2:0.8658741372026076 \n",
      "AUC LGB3:0.8658445909722999 \n",
      "AUC LGB4:0.8657425627540967 \n",
      "AUC LGB5:0.8657550293676526 \n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=3, random_state = 42,shuffle = True)\n",
    "\n",
    "oof_clf1 = np.zeros(len(train))\n",
    "oof_clf2 = np.zeros(len(train))\n",
    "oof_clf3 = np.zeros(len(train))\n",
    "oof_clf4 = np.zeros(len(train))\n",
    "oof_clf5 = np.zeros(len(train))\n",
    "\n",
    "oof_clf1_t = np.zeros(len(test))\n",
    "oof_clf2_t = np.zeros(len(test))\n",
    "oof_clf3_t = np.zeros(len(test))\n",
    "oof_clf4_t = np.zeros(len(test))\n",
    "oof_clf5_t = np.zeros(len(test))\n",
    "\n",
    "for (trn_idx, val_idx),test_val_idx in zip(kf.split(train.values, y.values),kf.split(test.values)):\n",
    "        test_val_idx = test_val_idx[0]\n",
    "        # print(\"fold n°{}\".format(fold_))        \n",
    "        \n",
    "        lgb_train = lgb.Dataset(train[FEATS].iloc[trn_idx], label = y.iloc[trn_idx]) # lgb.Dataset(train[FEATS], y_train)\n",
    "        lgb_valid = lgb.Dataset(train[FEATS].iloc[val_idx], label = y.iloc[val_idx]) # lgb.Dataset(valid[FEATS], y_valid)\n",
    "        \n",
    "        # num_round? 1000~ 10000\n",
    "        model1 = lgb.train(param1, lgb_train, 1000, valid_sets=[lgb_valid], early_stopping_rounds=100,verbose_eval=100, categorical_feature = cat_cols)\n",
    "        model2 = lgb.train(param2, lgb_train, 10000, valid_sets=[lgb_valid], early_stopping_rounds=100,verbose_eval=100, categorical_feature = cat_cols)\n",
    "        model3 = lgb.train(param3, lgb_train, 10000, valid_sets=[lgb_valid], early_stopping_rounds=100,verbose_eval=100, categorical_feature = cat_cols)\n",
    "        model4 = lgb.train(param4, lgb_train, 10000, valid_sets=[lgb_valid], early_stopping_rounds=100,verbose_eval=100, categorical_feature = cat_cols)\n",
    "        model5 = lgb.train(param5, lgb_train, 10000, valid_sets=[lgb_valid], early_stopping_rounds=100,verbose_eval=100, categorical_feature = cat_cols)            \n",
    "        \n",
    "        oof_clf1[val_idx] = model1.predict(train[FEATS].iloc[val_idx])\n",
    "        oof_clf2[val_idx] = model2.predict(train[FEATS].iloc[val_idx])\n",
    "        oof_clf3[val_idx] = model3.predict(train[FEATS].iloc[val_idx])\n",
    "        oof_clf4[val_idx] = model4.predict(train[FEATS].iloc[val_idx])\n",
    "        oof_clf5[val_idx] = model5.predict(train[FEATS].iloc[val_idx])\n",
    "        \n",
    "        oof_clf1_t[test_val_idx] = model1.predict(test[FEATS].iloc[test_val_idx])\n",
    "        oof_clf2_t[test_val_idx] = model2.predict(test[FEATS].iloc[test_val_idx])\n",
    "        oof_clf3_t[test_val_idx] = model3.predict(test[FEATS].iloc[test_val_idx])\n",
    "        oof_clf4_t[test_val_idx] = model4.predict(test[FEATS].iloc[test_val_idx])\n",
    "        oof_clf5_t[test_val_idx] = model5.predict(test[FEATS].iloc[test_val_idx])\n",
    "        \n",
    "        # print('Fold no: {}'.format(fold_))\n",
    "        print(\"AUC LGB1:{} \".format(metrics.roc_auc_score(y.iloc[val_idx], oof_clf1[val_idx])))\n",
    "        print(\"AUC LGB2:{} \".format(metrics.roc_auc_score(y.iloc[val_idx], oof_clf2[val_idx])))\n",
    "        print(\"AUC LGB3:{} \".format(metrics.roc_auc_score(y.iloc[val_idx], oof_clf3[val_idx])))\n",
    "        print(\"AUC LGB4:{} \".format(metrics.roc_auc_score(y.iloc[val_idx], oof_clf4[val_idx])))\n",
    "        print(\"AUC LGB5:{} \".format(metrics.roc_auc_score(y.iloc[val_idx], oof_clf5[val_idx]))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fa96f2b-c9b8-482e-9c5a-bf6d57f10479",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_train = pd.DataFrame( {\n",
    "        'LGBM1': oof_clf1.ravel(),\n",
    "        'LGBM2': oof_clf2.ravel(),\n",
    "        'LGBM3': oof_clf3.ravel(),\n",
    "        'LGBM4' :oof_clf4.ravel(),\n",
    "        'LGBM5': oof_clf5.ravel(),\n",
    "    })\n",
    "\n",
    "Final_test = pd.DataFrame( {\n",
    "        'LGBM1': oof_clf1_t.ravel(),\n",
    "        'LGBM2': oof_clf2_t.ravel(),\n",
    "        'LGBM3': oof_clf3_t.ravel(),\n",
    "        'LGBM4' :oof_clf4_t.ravel(),\n",
    "        'LGBM5': oof_clf5_t.ravel(),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3b5435a-ab5d-4f37-90d7-0655c15f2da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'objective': 'binary',\n",
    "                 'boosting': 'gbdt',\n",
    "                 'random_state': 42,\n",
    "                 'metric': 'auc',\n",
    "                 'num_threads': -1,\n",
    "                 'learning_rate' : 0.1,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8327116-8598-4455-b695-9d18c7e88675",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1323032, number of negative: 697732\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009170 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 2020764, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654719 -> initscore=0.639846\n",
      "[LightGBM] [Info] Start training from score 0.639846\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's auc: 0.866285\n",
      "[200]\tvalid_0's auc: 0.866256\n",
      "Early stopping, best iteration is:\n",
      "[48]\tvalid_0's auc: 0.866288\n",
      "AUC LGB1:0.8662878705802779 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1322521, number of negative: 698244\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018642 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 2020765, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654466 -> initscore=0.638726\n",
      "[LightGBM] [Info] Start training from score 0.638726\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's auc: 0.866342\n",
      "[200]\tvalid_0's auc: 0.866312\n",
      "Early stopping, best iteration is:\n",
      "[42]\tvalid_0's auc: 0.866358\n",
      "AUC LGB1:0.8663577706457455 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1323005, number of negative: 697760\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027044 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 2020765, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654705 -> initscore=0.639786\n",
      "[LightGBM] [Info] Start training from score 0.639786\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's auc: 0.866262\n",
      "[200]\tvalid_0's auc: 0.866221\n",
      "Early stopping, best iteration is:\n",
      "[46]\tvalid_0's auc: 0.866275\n",
      "AUC LGB1:0.8662754525995435 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1323025, number of negative: 697740\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010923 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 2020765, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654715 -> initscore=0.639830\n",
      "[LightGBM] [Info] Start training from score 0.639830\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's auc: 0.866436\n",
      "[200]\tvalid_0's auc: 0.8664\n",
      "Early stopping, best iteration is:\n",
      "[50]\tvalid_0's auc: 0.866447\n",
      "AUC LGB1:0.866446664580775 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1322769, number of negative: 697996\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010477 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 2020765, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654588 -> initscore=0.639269\n",
      "[LightGBM] [Info] Start training from score 0.639269\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's auc: 0.866603\n",
      "[200]\tvalid_0's auc: 0.866576\n",
      "Early stopping, best iteration is:\n",
      "[70]\tvalid_0's auc: 0.866613\n",
      "AUC LGB1:0.8666129070944879 \n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, random_state = 42,shuffle = True)\n",
    "\n",
    "Final_oof_train = np.zeros(len(train))\n",
    "Final_oof_test = np.zeros(len(test))\n",
    "\n",
    "for (trn_idx, val_idx), test_val_idx in zip(kf.split(Final_train.values, y.values), kf.split(Final_test.values)):\n",
    "        # print(\"fold n°{}\".format(fold_))\n",
    "        test_val_idx = test_val_idx[0]\n",
    "        lgb_train = lgb.Dataset(Final_train.iloc[trn_idx], label = y.iloc[trn_idx])\n",
    "        lgb_valid = lgb.Dataset(Final_train.iloc[val_idx], label = y.iloc[val_idx])\n",
    "        \n",
    "        model = lgb.train(param, lgb_train, 1000, valid_sets=[lgb_valid], early_stopping_rounds=200,verbose_eval=100)\n",
    "                \n",
    "        Final_oof_train[val_idx] = model.predict(Final_train.iloc[val_idx])        \n",
    "        \n",
    "        Final_oof_test[test_val_idx] = model.predict(Final_test.iloc[test_val_idx])\n",
    "        \n",
    "        # print('Fold no: {}'.format(fold_))\n",
    "        print(\"AUC LGB1:{} \".format(metrics.roc_auc_score(y.iloc[val_idx], Final_oof_train[val_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afa5b7cd-378d-4a54-afb9-620b1681b01c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.46788251, 0.20957201, 0.13674772, 0.91817273, 0.23295179,\n",
       "       0.81985044, 0.09541078, 0.05498945, 0.12050479, 0.60411493,\n",
       "       0.13212109, 0.28056417, 0.79540276, 0.07635603, 0.65956372,\n",
       "       0.91746535, 0.14974235, 0.80022905, 0.92674072, 0.15032902,\n",
       "       0.90005253, 0.5752921 , 0.37832062, 0.18393309, 0.19942241,\n",
       "       0.64169827, 0.5649868 , 0.8554898 , 0.32200937, 0.62807214,\n",
       "       0.67421773, 0.36498613, 0.46744211, 0.06375022, 0.57880483,\n",
       "       0.9115811 , 0.13301957, 0.2414413 , 0.07672676, 0.05001478,\n",
       "       0.2145559 , 0.06192513, 0.1900118 , 0.16961278, 0.59006836,\n",
       "       0.62871669, 0.64087499, 0.196992  , 0.61839725, 0.66717604,\n",
       "       0.58467226, 0.21089034, 0.20737395, 0.11535902, 0.23322659,\n",
       "       0.59037103, 0.17229198, 0.89233528, 0.1060587 , 0.0322221 ,\n",
       "       0.81065495, 0.79896869, 0.91139298, 0.22616395, 0.11442982,\n",
       "       0.19750195, 0.66131036, 0.23651547, 0.24302509, 0.17739713,\n",
       "       0.94470951, 0.6437222 , 0.02077158, 0.22564027, 0.65845905,\n",
       "       0.4277277 , 0.21090489, 0.71229791, 0.03603759, 0.54267625,\n",
       "       0.30977281, 0.52055655, 0.45838395, 0.57687993, 0.79524234,\n",
       "       0.10711691, 0.40189882, 0.01101145, 0.19641079, 0.2020732 ,\n",
       "       0.28944132, 0.0422882 , 0.22117298, 0.7018394 , 0.77467272,\n",
       "       0.45333367, 0.03099188, 0.79487726, 0.91386217, 0.90690931,\n",
       "       0.09688013, 0.0916594 , 0.38625909, 0.20770533, 0.12090405,\n",
       "       0.66956789, 0.10807694, 0.77326618, 0.03430744, 0.59577398,\n",
       "       0.27528739, 0.96309278, 0.07409316, 0.12301855, 0.04912859,\n",
       "       0.75995491, 0.66322181, 0.27791607, 0.15992431, 0.30393037,\n",
       "       0.68388693, 0.87496502, 0.93060994, 0.0471473 , 0.00802658,\n",
       "       0.08042207, 0.66877096, 0.07209501, 0.82812379, 0.05458489,\n",
       "       0.92360235, 0.11510222, 0.90089642, 0.23980183, 0.58054749,\n",
       "       0.3320138 , 0.05465412, 0.29897834, 0.11211917, 0.56805433,\n",
       "       0.50635102, 0.31500053, 0.24193754, 0.25242843, 0.52226563,\n",
       "       0.10861018, 0.84087005, 0.90120152, 0.06091519, 0.21902374,\n",
       "       0.02011771, 0.34888437, 0.85152228, 0.0130235 , 0.18356296,\n",
       "       0.28173626, 0.15654188, 0.30752641, 0.37931836, 0.11845446,\n",
       "       0.06529495, 0.11422646, 0.25858194, 0.10203524, 0.86107537,\n",
       "       0.32731903, 0.01366733, 0.12652128, 0.53096413, 0.3051531 ,\n",
       "       0.6534726 , 0.14580407, 0.0607912 , 0.01145085, 0.19840207,\n",
       "       0.78148757, 0.75737406, 0.01462818, 0.05110196, 0.1776516 ,\n",
       "       0.26105814, 0.12090405, 0.39455102, 0.18985801, 0.79811087,\n",
       "       0.29526099, 0.35229874, 0.29130435, 0.88917324, 0.05390633,\n",
       "       0.32081439, 0.12090422, 0.31932365, 0.71230978, 0.49325714,\n",
       "       0.4089277 , 0.78163366, 0.66738962, 0.88686533, 0.05203784,\n",
       "       0.26552508, 0.56348065, 0.20629691, 0.50551539, 0.0476933 ,\n",
       "       0.87832563, 0.01509624, 0.06038751, 0.26880293, 0.005073  ,\n",
       "       0.09611104, 0.84402815, 0.05615595, 0.07200352, 0.26699434,\n",
       "       0.11255647, 0.91509204, 0.73982843, 0.7767931 , 0.79820755,\n",
       "       0.24946684, 0.51243498, 0.05702926, 0.97965414, 0.84628045,\n",
       "       0.17171286, 0.52309235, 0.02904676, 0.12298452, 0.04232945,\n",
       "       0.06991215, 0.41598398, 0.76321748, 0.16088857, 0.70739177,\n",
       "       0.57428756, 0.15989268, 0.04311197, 0.16630851, 0.39994527,\n",
       "       0.45695773, 0.0790604 , 0.0579143 , 0.77737233, 0.89784229,\n",
       "       0.24042246, 0.22130057, 0.36957725, 0.03536461, 0.11007977,\n",
       "       0.89132596, 0.65106035, 0.31523011, 0.69795529, 0.51846334,\n",
       "       0.04188021, 0.6334086 , 0.43423639, 0.14250984, 0.28192171,\n",
       "       0.00602228, 0.19436838, 0.01110892, 0.05555415, 0.17040176,\n",
       "       0.18430951, 0.22157761, 0.80445724, 0.01381525, 0.02471927,\n",
       "       0.04219465, 0.12051804, 0.12409762, 0.73223645, 0.69943425,\n",
       "       0.03345049, 0.66556061, 0.02627939, 0.07551446, 0.71054877,\n",
       "       0.51915288, 0.09798419, 0.7997815 , 0.90122967, 0.44493625,\n",
       "       0.11403918, 0.12318594, 0.0476933 , 0.44469392, 0.71214814,\n",
       "       0.3603902 , 0.17043644, 0.05458489, 0.00163925, 0.30100509,\n",
       "       0.26899489, 0.3257758 , 0.58605741, 0.38434562, 0.18064778,\n",
       "       0.21984383, 0.11338616, 0.02514924, 0.84285712, 0.27419824,\n",
       "       0.03969453, 0.30489334, 0.82633422, 0.06917429, 0.21149921,\n",
       "       0.08985944, 0.02493066, 0.62190047, 0.31910609, 0.31054379,\n",
       "       0.6550866 , 0.95159158, 0.40943654, 0.23301988, 0.43150526,\n",
       "       0.21868079, 0.19854118, 0.66823803, 0.06386839, 0.50704392,\n",
       "       0.04340365, 0.65887005, 0.07310285, 0.37069931, 0.8100298 ,\n",
       "       0.94484559, 0.04827314, 0.12575136, 0.20543241, 0.78699526,\n",
       "       0.2217646 , 0.0523866 , 0.30016909, 0.81921605, 0.66491688,\n",
       "       0.61170259, 0.28907251, 0.30333818, 0.03803331, 0.24706512,\n",
       "       0.86270166, 0.38110773, 0.03694747, 0.2014597 , 0.27821491,\n",
       "       0.52946316, 0.31532148, 0.47765266, 0.46005006, 0.46727875,\n",
       "       0.28883098, 0.67446191, 0.21231617, 0.0688333 , 0.15177986,\n",
       "       0.11333043, 0.00163925, 0.07688285, 0.07828646, 0.09141167,\n",
       "       0.53324137, 0.6351535 , 0.09980801, 0.15027045, 0.06322483,\n",
       "       0.08737659, 0.65831533, 0.79966406, 0.94935743, 0.10892356,\n",
       "       0.75716798, 0.38205165, 0.20778036, 0.72393216, 0.15981814,\n",
       "       0.09234609, 0.87281499, 0.43314873, 0.1118869 , 0.17171871,\n",
       "       0.1891416 , 0.27512978, 0.78818719, 0.93786331, 0.39423474,\n",
       "       0.20405086, 0.02034661, 0.17151314, 0.04277113, 0.89794229,\n",
       "       0.01194761, 0.29163848, 0.71839689, 0.11754158, 0.09018464,\n",
       "       0.84874272, 0.54233987, 0.16202947, 0.06632962, 0.06959778,\n",
       "       0.25881992, 0.90000002, 0.00163925, 0.10263445, 0.13306643,\n",
       "       0.26967163, 0.00163925, 0.18519573, 0.15632872, 0.07786112,\n",
       "       0.20294172, 0.74820151, 0.36784367, 0.51023489, 0.32399608,\n",
       "       0.81035944, 0.39504693, 0.38145442, 0.48799921, 0.52190156,\n",
       "       0.04595856, 0.67132125, 0.30866456, 0.56168099, 0.21258153,\n",
       "       0.60184332, 0.00163925, 0.62380963, 0.72777777, 0.46416168,\n",
       "       0.4237725 , 0.12017119, 0.22756918, 0.61755392, 0.14335922,\n",
       "       0.06522191, 0.03041732, 0.1324489 , 0.51896371, 0.02146659,\n",
       "       0.82949173, 0.05025757, 0.598132  , 0.05932634, 0.23338312,\n",
       "       0.28476239, 0.62575064, 0.38855136, 0.19738733, 0.07931214,\n",
       "       0.17012868, 0.10428158, 0.53228367, 0.2399647 , 0.0424739 ,\n",
       "       0.71791652, 0.86461208, 0.06082542, 0.21798562, 0.38473181,\n",
       "       0.12090405, 0.71101937, 0.02367414, 0.13845466, 0.10648327,\n",
       "       0.00259886, 0.09697225, 0.41206983, 0.59264871, 0.02471927,\n",
       "       0.17745507, 0.13699011, 0.01639227, 0.66685136, 0.46185926,\n",
       "       0.14212639, 0.35441402, 0.0169333 , 0.04377615, 0.78449765,\n",
       "       0.05203784, 0.16692206, 0.03723059, 0.14825588, 0.01101145,\n",
       "       0.00602228, 0.12285394, 0.34958031, 0.08978454, 0.22347928,\n",
       "       0.02098314, 0.13891794, 0.4672094 , 0.15571902, 0.59075885,\n",
       "       0.37825526, 0.11754523, 0.09806586, 0.00163925, 0.18431046,\n",
       "       0.09431874, 0.48390853, 0.61084746, 0.24937026, 0.06374746,\n",
       "       0.02070805, 0.12622081, 0.56773965, 0.22304048, 0.40029802,\n",
       "       0.40988733, 0.7750568 , 0.06386839, 0.15245136, 0.07781584,\n",
       "       0.45976469, 0.57568602, 0.02687715, 0.01332606, 0.75020914,\n",
       "       0.12265648, 0.61844051, 0.1111853 , 0.68542308, 0.42237142,\n",
       "       0.61590773, 0.01911538, 0.14666271, 0.21627377, 0.06386839,\n",
       "       0.35650178, 0.53049659, 0.25431209, 0.00657013, 0.12495685,\n",
       "       0.94472197, 0.28257792, 0.54708291, 0.08381811, 0.20938381,\n",
       "       0.93078039, 0.62652023, 0.84684537, 0.67295932, 0.52086082,\n",
       "       0.18046587, 0.10844143, 0.26216403, 0.18376932, 0.05555415,\n",
       "       0.26386102, 0.25717005, 0.39241225, 0.25154463, 0.20375146,\n",
       "       0.32995541, 0.4950337 , 0.23828647, 0.01509624, 0.05053401,\n",
       "       0.25788939, 0.00656729, 0.91548673, 0.05303317, 0.03262656,\n",
       "       0.0824228 , 0.08759725, 0.00163925, 0.05862545, 0.26586642,\n",
       "       0.05735466, 0.11758117, 0.19942102, 0.46559332, 0.41419962,\n",
       "       0.23836575, 0.55723869, 0.7348614 , 0.03627259, 0.13647824,\n",
       "       0.21936251, 0.60041992, 0.18824086, 0.20391958, 0.04192587,\n",
       "       0.03929249, 0.41667722, 0.41148333, 0.19947164, 0.02359697,\n",
       "       0.0130235 , 0.89731923, 0.20659755, 0.11846162, 0.60681017,\n",
       "       0.78401241, 0.13645905, 0.30072239, 0.15985264, 0.71501927,\n",
       "       0.56681386, 0.1267146 , 0.65169127, 0.03434238, 0.73678495,\n",
       "       0.22588446, 0.12138467, 0.81178234, 0.00163925, 0.20786429,\n",
       "       0.20805384, 0.71025705, 0.00163925, 0.1163237 , 0.00163925,\n",
       "       0.13217336, 0.0344822 , 0.02095331, 0.01728769, 0.05110196,\n",
       "       0.00259886, 0.00415024, 0.71551885, 0.02618539, 0.06377097,\n",
       "       0.00163925, 0.40110461, 0.32961839, 0.25940803, 0.3459259 ,\n",
       "       0.01248217, 0.13725872, 0.11664521, 0.29817649, 0.00602228,\n",
       "       0.28165808, 0.24360965, 0.02000339, 0.01110892, 0.08064915,\n",
       "       0.04691632, 0.06541705, 0.72736241, 0.09050574, 0.11416471,\n",
       "       0.01460726, 0.52996348, 0.16084165, 0.63679696, 0.03393085,\n",
       "       0.10217642, 0.12965754, 0.77485199, 0.26224453, 0.01248217,\n",
       "       0.02409636, 0.32896979, 0.03323327, 0.08865059, 0.3314841 ,\n",
       "       0.29910687, 0.04173117, 0.32297723, 0.21844232, 0.18147939,\n",
       "       0.02760067, 0.00657013, 0.31022175, 0.14694385, 0.15507712,\n",
       "       0.67893969, 0.00163925, 0.45577195, 0.20097864, 0.05144478,\n",
       "       0.00163925, 0.10651856, 0.65907814, 0.17010664, 0.18368546,\n",
       "       0.02843319, 0.20561435, 0.22587165, 0.34172715, 0.14291137,\n",
       "       0.56484365, 0.06948009, 0.02532261, 0.03099209, 0.22037935,\n",
       "       0.00163925, 0.11372476, 0.00163925, 0.02274889, 0.28092746,\n",
       "       0.12538985, 0.71893739, 0.29633331, 0.26325352, 0.02164551,\n",
       "       0.00963453, 0.00163925, 0.07650323, 0.01110892, 0.00163925,\n",
       "       0.49157173, 0.01777159, 0.36895826, 0.05778829, 0.00163925,\n",
       "       0.12939849, 0.29792453, 0.14258921, 0.0216465 , 0.01728769,\n",
       "       0.13382199, 0.26917039, 0.02146659, 0.25732159, 0.43535149,\n",
       "       0.02760067, 0.00823387, 0.00656729, 0.12758131, 0.01040396,\n",
       "       0.80561004, 0.00163925, 0.08566422, 0.07447213, 0.1072125 ,\n",
       "       0.21621331, 0.17429452, 0.00656729, 0.02770026, 0.00163925,\n",
       "       0.26240592, 0.1637449 , 0.27034192, 0.39721263])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Final_oof_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66a99cb3-1a1c-48d4-82b0-652a4f385c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_to_csv(preds, name:str):\n",
    "    \n",
    "    result = []\n",
    "    for n,i in enumerate(preds):\n",
    "        row = {}    \n",
    "        row['id'] = n\n",
    "        row['prediction'] = i\n",
    "        result.append(row)\n",
    "    pd.DataFrame(result).to_csv(f'output/{name}.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e9d7861-8ecc-4520-81bf-6d930f32a634",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_to_csv(Final_oof_test,'Ensemble2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
