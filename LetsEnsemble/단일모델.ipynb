{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92d257cd-824a-4921-8b21-c14bada43824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/sarmat/lgbm-stacking-example/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1cc2c9a-80ee-4932-b759-7f49c34b9ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from dataset import custom_train_test_split, make_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84f4ceb8-7f94-43ca-90e5-1fa66095051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "param1 = {   \n",
    "            'boosting': 'gbdt',\n",
    "            'objective': 'binary',\n",
    "            'num_leaves' : 30,\n",
    "            'metric' : 'binary_logloss',\n",
    "            'learning_rate' : 0.1\n",
    "            \n",
    "            }\n",
    "\n",
    "param2 = {   \n",
    "            'boosting': 'gbdt', \n",
    "            # 'max_depth' : 10,\n",
    "            'objective': 'binary',\n",
    "            'num_leaves' : 50, # 30 ~ 60\n",
    "            'metric' : 'binary_logloss',\n",
    "            'learning_rate' : 0.1 #  0.01~0.3\n",
    "            }\n",
    "\n",
    "param3 = {   \n",
    "            'boosting': 'gbdt',\n",
    "            'objective': 'binary',\n",
    "            'num_leaves' : 48,\n",
    "            'metric' : 'binary_logloss',\n",
    "            # 'learning_rate' : 0.005\n",
    "            }\n",
    "\n",
    "param4 = {   \n",
    "            'boosting': 'gbdt',\n",
    "            'objective': 'binary',\n",
    "            'num_leaves' : 60,\n",
    "            'metric' : 'binary_logloss',\n",
    "            # 'learning_rate' : 0.005\n",
    "            }\n",
    "\n",
    "param5 = {   \n",
    "            'boosting': 'gbdt',\n",
    "            'objective': 'binary',\n",
    "            'num_leaves' : 70,\n",
    "            'metric' : 'binary_logloss',\n",
    "            # 'learning_rate' : 0.01\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee7f6473-3745-4b22-9fbd-f6dfe18b4a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['i_head', 'i_mid','i_tail', 'hour', 'dow']\n",
    "cont_cols = [                        \n",
    "        'user_correct_answer',\n",
    "        'user_total_answer',\n",
    "        'user_acc',            \n",
    "        't_elapsed',            \n",
    "        'cum_correct',\n",
    "        'last_problem',\n",
    "        'head_term',\n",
    "        # 'left_asymptote',\n",
    "        'elo_prob',\n",
    "        'pkt',\n",
    "        'u_head_mean',\n",
    "        'u_head_count',\n",
    "        'u_head_std',\n",
    "        'u_head_elapsed',\n",
    "        'i_mid_elapsed',\n",
    "        'i_mid_mean',\n",
    "        'i_mid_std',\n",
    "        'i_mid_sum',\n",
    "        'i_mid_count',\n",
    "        'i_mid_tag_count',\n",
    "        'assessment_mean',\n",
    "        'assessment_sum',\n",
    "        # 'assessment_std',\n",
    "        'tag_mean',\n",
    "        'tag_sum',\n",
    "        # 'tag_std',\n",
    "        'tail_mean',\n",
    "        'tail_sum',\n",
    "        # 'tail_std',\n",
    "        'hour_mean',\n",
    "        'hour_sum',\n",
    "        # 'hour_std',\n",
    "        'dow_mean',\n",
    "        'dow_sum',\n",
    "        # 'dow_std',\n",
    "        'tag_elapsed',\n",
    "        'tag_elapsed_o',\n",
    "        'tag_elapsed_x',\n",
    "        'assessment_elapsed',\n",
    "        'assessment_elapsed_o',\n",
    "        'assessment_elapsed_x',\n",
    "        'tail_elapsed',\n",
    "        'tail_elapsed_o',\n",
    "        'tail_elapsed_x']\n",
    "\n",
    "FEATS = cat_cols + cont_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9884ba83-6b5d-425b-be04-c0000a1f3c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('/opt/ml/level2-dkt-level2-recsys-08/data_pkl/all.pkl')\n",
    "y = train.answerCode\n",
    "# train, test = custom_train_test_split(train_data)\n",
    "# y_train, train, y_test, test = make_dataset(train, test)\n",
    "test = pd.read_pickle('/opt/ml/level2-dkt-level2-recsys-08/data_pkl/test_data.pkl')\n",
    "test = test[test.answerCode==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c404f1b8-04c6-49bf-843b-3f3485fd513a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/opt/conda/lib/python3.8/site-packages/lightgbm/basic.py:2068: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['dow', 'hour', 'i_head', 'i_mid', 'i_tail']\n",
      "  _log_warning('categorical_feature in Dataset is overridden.\\n'\n",
      "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1102675, number of negative: 581295\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038236 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6623\n",
      "[LightGBM] [Info] Number of data points in the train set: 1683970, number of used features: 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/opt/conda/lib/python3.8/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654807 -> initscore=0.640236\n",
      "[LightGBM] [Info] Start training from score 0.640236\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.43051\n",
      "[200]\tvalid_0's binary_logloss: 0.427988\n",
      "[300]\tvalid_0's binary_logloss: 0.427163\n",
      "[400]\tvalid_0's binary_logloss: 0.426864\n",
      "[500]\tvalid_0's binary_logloss: 0.426688\n",
      "[600]\tvalid_0's binary_logloss: 0.426551\n",
      "[700]\tvalid_0's binary_logloss: 0.426398\n",
      "[800]\tvalid_0's binary_logloss: 0.426277\n",
      "[900]\tvalid_0's binary_logloss: 0.426296\n",
      "[1000]\tvalid_0's binary_logloss: 0.426253\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[974]\tvalid_0's binary_logloss: 0.426219\n",
      "[LightGBM] [Info] Number of positive: 1102675, number of negative: 581295\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041778 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6623\n",
      "[LightGBM] [Info] Number of data points in the train set: 1683970, number of used features: 43\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654807 -> initscore=0.640236\n",
      "[LightGBM] [Info] Start training from score 0.640236\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.429517\n",
      "[200]\tvalid_0's binary_logloss: 0.427446\n",
      "[300]\tvalid_0's binary_logloss: 0.426873\n",
      "[400]\tvalid_0's binary_logloss: 0.426565\n",
      "[500]\tvalid_0's binary_logloss: 0.426495\n",
      "[600]\tvalid_0's binary_logloss: 0.426396\n",
      "[700]\tvalid_0's binary_logloss: 0.426291\n",
      "[800]\tvalid_0's binary_logloss: 0.426243\n",
      "[900]\tvalid_0's binary_logloss: 0.426257\n",
      "Early stopping, best iteration is:\n",
      "[870]\tvalid_0's binary_logloss: 0.426211\n",
      "[LightGBM] [Info] Number of positive: 1102675, number of negative: 581295\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039342 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6623\n",
      "[LightGBM] [Info] Number of data points in the train set: 1683970, number of used features: 43\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654807 -> initscore=0.640236\n",
      "[LightGBM] [Info] Start training from score 0.640236\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.429101\n",
      "[200]\tvalid_0's binary_logloss: 0.427154\n",
      "[300]\tvalid_0's binary_logloss: 0.426746\n",
      "[400]\tvalid_0's binary_logloss: 0.426586\n",
      "[500]\tvalid_0's binary_logloss: 0.426446\n",
      "[600]\tvalid_0's binary_logloss: 0.426417\n",
      "[700]\tvalid_0's binary_logloss: 0.426372\n",
      "[800]\tvalid_0's binary_logloss: 0.426344\n",
      "Early stopping, best iteration is:\n",
      "[780]\tvalid_0's binary_logloss: 0.4263\n",
      "[LightGBM] [Info] Number of positive: 1102675, number of negative: 581295\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069976 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6623\n",
      "[LightGBM] [Info] Number of data points in the train set: 1683970, number of used features: 43\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654807 -> initscore=0.640236\n",
      "[LightGBM] [Info] Start training from score 0.640236\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.428162\n",
      "[200]\tvalid_0's binary_logloss: 0.426767\n",
      "[300]\tvalid_0's binary_logloss: 0.42657\n",
      "[400]\tvalid_0's binary_logloss: 0.426418\n",
      "[500]\tvalid_0's binary_logloss: 0.426432\n",
      "Early stopping, best iteration is:\n",
      "[401]\tvalid_0's binary_logloss: 0.42641\n",
      "[LightGBM] [Info] Number of positive: 1102675, number of negative: 581295\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057464 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6623\n",
      "[LightGBM] [Info] Number of data points in the train set: 1683970, number of used features: 43\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654807 -> initscore=0.640236\n",
      "[LightGBM] [Info] Start training from score 0.640236\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.427857\n",
      "[200]\tvalid_0's binary_logloss: 0.426621\n",
      "[300]\tvalid_0's binary_logloss: 0.426376\n",
      "[400]\tvalid_0's binary_logloss: 0.42624\n",
      "[500]\tvalid_0's binary_logloss: 0.426285\n",
      "Early stopping, best iteration is:\n",
      "[452]\tvalid_0's binary_logloss: 0.426213\n",
      "AUC LGB1:0.8655615787620669 \n",
      "AUC LGB2:0.8655457381537353 \n",
      "AUC LGB3:0.8654923739244257 \n",
      "AUC LGB4:0.865425796941456 \n",
      "AUC LGB5:0.8655451548138007 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/opt/conda/lib/python3.8/site-packages/lightgbm/basic.py:2068: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['dow', 'hour', 'i_head', 'i_mid', 'i_tail']\n",
      "  _log_warning('categorical_feature in Dataset is overridden.\\n'\n",
      "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1101885, number of negative: 582086\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036678 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6622\n",
      "[LightGBM] [Info] Number of data points in the train set: 1683971, number of used features: 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/opt/conda/lib/python3.8/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654337 -> initscore=0.638159\n",
      "[LightGBM] [Info] Start training from score 0.638159\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.429782\n",
      "[200]\tvalid_0's binary_logloss: 0.427361\n",
      "[300]\tvalid_0's binary_logloss: 0.426542\n",
      "[400]\tvalid_0's binary_logloss: 0.426205\n",
      "[500]\tvalid_0's binary_logloss: 0.425999\n",
      "[600]\tvalid_0's binary_logloss: 0.425907\n",
      "[700]\tvalid_0's binary_logloss: 0.42588\n",
      "[800]\tvalid_0's binary_logloss: 0.425796\n",
      "Early stopping, best iteration is:\n",
      "[786]\tvalid_0's binary_logloss: 0.425787\n",
      "[LightGBM] [Info] Number of positive: 1101885, number of negative: 582086\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055203 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6622\n",
      "[LightGBM] [Info] Number of data points in the train set: 1683971, number of used features: 43\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654337 -> initscore=0.638159\n",
      "[LightGBM] [Info] Start training from score 0.638159\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.428873\n",
      "[200]\tvalid_0's binary_logloss: 0.426854\n",
      "[300]\tvalid_0's binary_logloss: 0.426263\n",
      "[400]\tvalid_0's binary_logloss: 0.426014\n",
      "[500]\tvalid_0's binary_logloss: 0.425859\n",
      "[600]\tvalid_0's binary_logloss: 0.425824\n",
      "[700]\tvalid_0's binary_logloss: 0.42578\n",
      "[800]\tvalid_0's binary_logloss: 0.425851\n",
      "Early stopping, best iteration is:\n",
      "[711]\tvalid_0's binary_logloss: 0.425776\n",
      "[LightGBM] [Info] Number of positive: 1101885, number of negative: 582086\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.076845 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6622\n",
      "[LightGBM] [Info] Number of data points in the train set: 1683971, number of used features: 43\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654337 -> initscore=0.638159\n",
      "[LightGBM] [Info] Start training from score 0.638159\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.428112\n",
      "[200]\tvalid_0's binary_logloss: 0.426345\n",
      "[300]\tvalid_0's binary_logloss: 0.425721\n",
      "[400]\tvalid_0's binary_logloss: 0.425479\n",
      "[500]\tvalid_0's binary_logloss: 0.425339\n",
      "[600]\tvalid_0's binary_logloss: 0.425387\n",
      "Early stopping, best iteration is:\n",
      "[521]\tvalid_0's binary_logloss: 0.425317\n",
      "[LightGBM] [Info] Number of positive: 1101885, number of negative: 582086\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040504 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6622\n",
      "[LightGBM] [Info] Number of data points in the train set: 1683971, number of used features: 43\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654337 -> initscore=0.638159\n",
      "[LightGBM] [Info] Start training from score 0.638159\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.427494\n",
      "[200]\tvalid_0's binary_logloss: 0.426082\n",
      "[300]\tvalid_0's binary_logloss: 0.425818\n",
      "[400]\tvalid_0's binary_logloss: 0.425562\n",
      "[500]\tvalid_0's binary_logloss: 0.425561\n",
      "[600]\tvalid_0's binary_logloss: 0.425519\n",
      "Early stopping, best iteration is:\n",
      "[552]\tvalid_0's binary_logloss: 0.425493\n",
      "[LightGBM] [Info] Number of positive: 1101885, number of negative: 582086\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056965 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6622\n",
      "[LightGBM] [Info] Number of data points in the train set: 1683971, number of used features: 43\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654337 -> initscore=0.638159\n",
      "[LightGBM] [Info] Start training from score 0.638159\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.427106\n",
      "[200]\tvalid_0's binary_logloss: 0.426125\n",
      "[300]\tvalid_0's binary_logloss: 0.425814\n",
      "[400]\tvalid_0's binary_logloss: 0.42582\n",
      "Early stopping, best iteration is:\n",
      "[318]\tvalid_0's binary_logloss: 0.425776\n",
      "AUC LGB1:0.8655734706367643 \n",
      "AUC LGB2:0.8655580032655293 \n",
      "AUC LGB3:0.8658420000256191 \n",
      "AUC LGB4:0.8656923170578461 \n",
      "AUC LGB5:0.8655183166041627 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/opt/conda/lib/python3.8/site-packages/lightgbm/basic.py:2068: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['dow', 'hour', 'i_head', 'i_mid', 'i_tail']\n",
      "  _log_warning('categorical_feature in Dataset is overridden.\\n'\n",
      "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1102616, number of negative: 581355\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.046860 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6623\n",
      "[LightGBM] [Info] Number of data points in the train set: 1683971, number of used features: 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/opt/conda/lib/python3.8/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654771 -> initscore=0.640079\n",
      "[LightGBM] [Info] Start training from score 0.640079\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.430234\n",
      "[200]\tvalid_0's binary_logloss: 0.427705\n",
      "[300]\tvalid_0's binary_logloss: 0.426869\n",
      "[400]\tvalid_0's binary_logloss: 0.426439\n",
      "[500]\tvalid_0's binary_logloss: 0.426275\n",
      "[600]\tvalid_0's binary_logloss: 0.426104\n",
      "[700]\tvalid_0's binary_logloss: 0.425958\n",
      "[800]\tvalid_0's binary_logloss: 0.425907\n",
      "Early stopping, best iteration is:\n",
      "[778]\tvalid_0's binary_logloss: 0.425878\n",
      "[LightGBM] [Info] Number of positive: 1102616, number of negative: 581355\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041464 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6623\n",
      "[LightGBM] [Info] Number of data points in the train set: 1683971, number of used features: 43\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654771 -> initscore=0.640079\n",
      "[LightGBM] [Info] Start training from score 0.640079\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.429122\n",
      "[200]\tvalid_0's binary_logloss: 0.426934\n",
      "[300]\tvalid_0's binary_logloss: 0.426452\n",
      "[400]\tvalid_0's binary_logloss: 0.426235\n",
      "[500]\tvalid_0's binary_logloss: 0.426028\n",
      "[600]\tvalid_0's binary_logloss: 0.425951\n",
      "[700]\tvalid_0's binary_logloss: 0.425956\n",
      "[800]\tvalid_0's binary_logloss: 0.425915\n",
      "Early stopping, best iteration is:\n",
      "[756]\tvalid_0's binary_logloss: 0.425871\n",
      "[LightGBM] [Info] Number of positive: 1102616, number of negative: 581355\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040760 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6623\n",
      "[LightGBM] [Info] Number of data points in the train set: 1683971, number of used features: 43\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654771 -> initscore=0.640079\n",
      "[LightGBM] [Info] Start training from score 0.640079\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.428601\n",
      "[200]\tvalid_0's binary_logloss: 0.426801\n",
      "[300]\tvalid_0's binary_logloss: 0.426513\n",
      "[400]\tvalid_0's binary_logloss: 0.426252\n",
      "[500]\tvalid_0's binary_logloss: 0.426097\n",
      "[600]\tvalid_0's binary_logloss: 0.426034\n",
      "[700]\tvalid_0's binary_logloss: 0.425833\n",
      "[800]\tvalid_0's binary_logloss: 0.42594\n",
      "Early stopping, best iteration is:\n",
      "[705]\tvalid_0's binary_logloss: 0.42582\n",
      "[LightGBM] [Info] Number of positive: 1102616, number of negative: 581355\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041646 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6623\n",
      "[LightGBM] [Info] Number of data points in the train set: 1683971, number of used features: 43\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654771 -> initscore=0.640079\n",
      "[LightGBM] [Info] Start training from score 0.640079\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.427971\n",
      "[200]\tvalid_0's binary_logloss: 0.426488\n",
      "[300]\tvalid_0's binary_logloss: 0.426103\n",
      "[400]\tvalid_0's binary_logloss: 0.426081\n",
      "Early stopping, best iteration is:\n",
      "[318]\tvalid_0's binary_logloss: 0.426047\n",
      "[LightGBM] [Info] Number of positive: 1102616, number of negative: 581355\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043695 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6623\n",
      "[LightGBM] [Info] Number of data points in the train set: 1683971, number of used features: 43\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654771 -> initscore=0.640079\n",
      "[LightGBM] [Info] Start training from score 0.640079\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.427746\n",
      "[200]\tvalid_0's binary_logloss: 0.426357\n",
      "[300]\tvalid_0's binary_logloss: 0.426169\n",
      "[400]\tvalid_0's binary_logloss: 0.426002\n",
      "[500]\tvalid_0's binary_logloss: 0.426012\n",
      "Early stopping, best iteration is:\n",
      "[450]\tvalid_0's binary_logloss: 0.425953\n",
      "AUC LGB1:0.8658564971331191 \n",
      "AUC LGB2:0.8658741372026076 \n",
      "AUC LGB3:0.8658445909722999 \n",
      "AUC LGB4:0.8657425627540967 \n",
      "AUC LGB5:0.8657550293676526 \n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=3, random_state = 42,shuffle = True)\n",
    "\n",
    "oof_clf1 = np.zeros(len(train))\n",
    "oof_clf2 = np.zeros(len(train))\n",
    "oof_clf3 = np.zeros(len(train))\n",
    "oof_clf4 = np.zeros(len(train))\n",
    "oof_clf5 = np.zeros(len(train))\n",
    "\n",
    "oof_clf1_t = np.zeros(len(test))\n",
    "oof_clf2_t = np.zeros(len(test))\n",
    "oof_clf3_t = np.zeros(len(test))\n",
    "oof_clf4_t = np.zeros(len(test))\n",
    "oof_clf5_t = np.zeros(len(test))\n",
    "\n",
    "for (trn_idx, val_idx),test_val_idx in zip(kf.split(train.values, y.values),kf.split(test.values)):\n",
    "        test_val_idx = test_val_idx[0]\n",
    "        # print(\"fold n°{}\".format(fold_))        \n",
    "        \n",
    "        lgb_train = lgb.Dataset(train[FEATS].iloc[trn_idx], label = y.iloc[trn_idx]) # lgb.Dataset(train[FEATS], y_train)\n",
    "        lgb_valid = lgb.Dataset(train[FEATS].iloc[val_idx], label = y.iloc[val_idx]) # lgb.Dataset(valid[FEATS], y_valid)\n",
    "        \n",
    "        # num_round? 1000~ 10000\n",
    "        model1 = lgb.train(param1, lgb_train, 1000, valid_sets=[lgb_valid], early_stopping_rounds=100,verbose_eval=100, categorical_feature = cat_cols)\n",
    "        model2 = lgb.train(param2, lgb_train, 10000, valid_sets=[lgb_valid], early_stopping_rounds=100,verbose_eval=100, categorical_feature = cat_cols)\n",
    "        model3 = lgb.train(param3, lgb_train, 10000, valid_sets=[lgb_valid], early_stopping_rounds=100,verbose_eval=100, categorical_feature = cat_cols)\n",
    "        model4 = lgb.train(param4, lgb_train, 10000, valid_sets=[lgb_valid], early_stopping_rounds=100,verbose_eval=100, categorical_feature = cat_cols)\n",
    "        model5 = lgb.train(param5, lgb_train, 10000, valid_sets=[lgb_valid], early_stopping_rounds=100,verbose_eval=100, categorical_feature = cat_cols)            \n",
    "        \n",
    "        oof_clf1[val_idx] = model1.predict(train[FEATS].iloc[val_idx])\n",
    "        oof_clf2[val_idx] = model2.predict(train[FEATS].iloc[val_idx])\n",
    "        oof_clf3[val_idx] = model3.predict(train[FEATS].iloc[val_idx])\n",
    "        oof_clf4[val_idx] = model4.predict(train[FEATS].iloc[val_idx])\n",
    "        oof_clf5[val_idx] = model5.predict(train[FEATS].iloc[val_idx])\n",
    "        \n",
    "        oof_clf1_t[test_val_idx] = model1.predict(test[FEATS].iloc[test_val_idx])\n",
    "        oof_clf2_t[test_val_idx] = model2.predict(test[FEATS].iloc[test_val_idx])\n",
    "        oof_clf3_t[test_val_idx] = model3.predict(test[FEATS].iloc[test_val_idx])\n",
    "        oof_clf4_t[test_val_idx] = model4.predict(test[FEATS].iloc[test_val_idx])\n",
    "        oof_clf5_t[test_val_idx] = model5.predict(test[FEATS].iloc[test_val_idx])\n",
    "        \n",
    "        # print('Fold no: {}'.format(fold_))\n",
    "        print(\"AUC LGB1:{} \".format(metrics.roc_auc_score(y.iloc[val_idx], oof_clf1[val_idx])))\n",
    "        print(\"AUC LGB2:{} \".format(metrics.roc_auc_score(y.iloc[val_idx], oof_clf2[val_idx])))\n",
    "        print(\"AUC LGB3:{} \".format(metrics.roc_auc_score(y.iloc[val_idx], oof_clf3[val_idx])))\n",
    "        print(\"AUC LGB4:{} \".format(metrics.roc_auc_score(y.iloc[val_idx], oof_clf4[val_idx])))\n",
    "        print(\"AUC LGB5:{} \".format(metrics.roc_auc_score(y.iloc[val_idx], oof_clf5[val_idx]))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fa96f2b-c9b8-482e-9c5a-bf6d57f10479",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_train = pd.DataFrame( {\n",
    "        'LGBM1': oof_clf1.ravel(),\n",
    "        'LGBM2': oof_clf2.ravel(),\n",
    "        'LGBM3': oof_clf3.ravel(),\n",
    "        'LGBM4' :oof_clf4.ravel(),\n",
    "        'LGBM5': oof_clf5.ravel(),\n",
    "    })\n",
    "\n",
    "Final_test = pd.DataFrame( {\n",
    "        'LGBM1': oof_clf1_t.ravel(),\n",
    "        'LGBM2': oof_clf2_t.ravel(),\n",
    "        'LGBM3': oof_clf3_t.ravel(),\n",
    "        'LGBM4' :oof_clf4_t.ravel(),\n",
    "        'LGBM5': oof_clf5_t.ravel(),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3b5435a-ab5d-4f37-90d7-0655c15f2da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'objective': 'binary',\n",
    "                 'boosting': 'gbdt',\n",
    "                 'random_state': 42,\n",
    "                 'metric': 'auc',\n",
    "                 'num_threads': -1,\n",
    "                 'learning_rate' : 0.1,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8327116-8598-4455-b695-9d18c7e88675",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1323032, number of negative: 697732\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009170 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 2020764, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654719 -> initscore=0.639846\n",
      "[LightGBM] [Info] Start training from score 0.639846\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's auc: 0.866285\n",
      "[200]\tvalid_0's auc: 0.866256\n",
      "Early stopping, best iteration is:\n",
      "[48]\tvalid_0's auc: 0.866288\n",
      "AUC LGB1:0.8662878705802779 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1322521, number of negative: 698244\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018642 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 2020765, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654466 -> initscore=0.638726\n",
      "[LightGBM] [Info] Start training from score 0.638726\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's auc: 0.866342\n",
      "[200]\tvalid_0's auc: 0.866312\n",
      "Early stopping, best iteration is:\n",
      "[42]\tvalid_0's auc: 0.866358\n",
      "AUC LGB1:0.8663577706457455 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1323005, number of negative: 697760\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027044 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 2020765, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654705 -> initscore=0.639786\n",
      "[LightGBM] [Info] Start training from score 0.639786\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's auc: 0.866262\n",
      "[200]\tvalid_0's auc: 0.866221\n",
      "Early stopping, best iteration is:\n",
      "[46]\tvalid_0's auc: 0.866275\n",
      "AUC LGB1:0.8662754525995435 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1323025, number of negative: 697740\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010923 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 2020765, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654715 -> initscore=0.639830\n",
      "[LightGBM] [Info] Start training from score 0.639830\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's auc: 0.866436\n",
      "[200]\tvalid_0's auc: 0.8664\n",
      "Early stopping, best iteration is:\n",
      "[50]\tvalid_0's auc: 0.866447\n",
      "AUC LGB1:0.866446664580775 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1322769, number of negative: 697996\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010477 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 2020765, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654588 -> initscore=0.639269\n",
      "[LightGBM] [Info] Start training from score 0.639269\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's auc: 0.866603\n",
      "[200]\tvalid_0's auc: 0.866576\n",
      "Early stopping, best iteration is:\n",
      "[70]\tvalid_0's auc: 0.866613\n",
      "AUC LGB1:0.8666129070944879 \n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, random_state = 42,shuffle = True)\n",
    "\n",
    "Final_oof_train = np.zeros(len(train))\n",
    "Final_oof_test = np.zeros(len(test))\n",
    "\n",
    "for (trn_idx, val_idx), test_val_idx in zip(kf.split(Final_train.values, y.values), kf.split(Final_test.values)):\n",
    "        # print(\"fold n°{}\".format(fold_))\n",
    "        test_val_idx = test_val_idx[0]\n",
    "        lgb_train = lgb.Dataset(Final_train.iloc[trn_idx], label = y.iloc[trn_idx])\n",
    "        lgb_valid = lgb.Dataset(Final_train.iloc[val_idx], label = y.iloc[val_idx])\n",
    "        \n",
    "        model = lgb.train(param, lgb_train, 1000, valid_sets=[lgb_valid], early_stopping_rounds=200,verbose_eval=100)\n",
    "                \n",
    "        Final_oof_train[val_idx] = model.predict(Final_train.iloc[val_idx])        \n",
    "        \n",
    "        Final_oof_test[test_val_idx] = model.predict(Final_test.iloc[test_val_idx])\n",
    "        \n",
    "        # print('Fold no: {}'.format(fold_))\n",
    "        print(\"AUC LGB1:{} \".format(metrics.roc_auc_score(y.iloc[val_idx], Final_oof_train[val_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "afa5b7cd-378d-4a54-afb9-620b1681b01c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "744"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Final_oof_test.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66a99cb3-1a1c-48d4-82b0-652a4f385c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_to_csv(preds, name:str):\n",
    "    \n",
    "    result = []\n",
    "    for n,i in enumerate(preds):\n",
    "        row = {}    \n",
    "        row['id'] = n\n",
    "        row['prediction'] = i\n",
    "        result.append(row)\n",
    "    pd.DataFrame(result).to_csv(f'output/{name}.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e9d7861-8ecc-4520-81bf-6d930f32a634",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, datetime, timezone, timedelta\n",
    "\n",
    "KST = timezone(timedelta(hours=9))\n",
    "time_record = datetime.now(KST)\n",
    "_day = str(time_record)[:10]\n",
    "_time = str(time_record.time())[:8]\n",
    "now_time = _day+'_'+_time\n",
    "\n",
    "test_to_csv(Final_oof_test,f'Ensemble_{now_time}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
